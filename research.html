<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Dejan Milojevic</title>
        <meta name="description" content="">
        <!--
    	Volton Template
    	http://www.templatemo.com/tm-441-volton
        -->
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" href="css/normalize.css">
        <link rel="stylesheet" href="css/font-awesome.css">
        <link rel="stylesheet" href="css/academicons.css">
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/templatemo-style.css">

        <link rel="apple-touch-icon" sizes="180x180" href="/dejanfaceiconnew180.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/dejanfaceiconnew32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/dejanfaceiconnew16.png">
        <link rel="manifest" href="/manifest.json?v=9ByQ8gbazn">
        <link rel="mask-icon" href="/dejanfaceiconnew.svg" color="#8c1515">
        <link rel="shortcut icon" href="/dejanfaceiconnew.ico">
        <meta name="msapplication-TileColor" content="#8c1515">
        <meta name="msapplication-TileImage" content="/dejanfaceiconnew144.png?v=9ByQ8gbazn">
        <meta name="theme-color" content="#ffffff">

        <script src="js/vendor/modernizr-2.6.2.min.js"></script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-88631709-1', 'auto');
            ga('send', 'pageview');
        </script>
    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->

        <div class="responsive-header visible-xs visible-sm">
            <div class="container">
                <div class="row" style="margin-right: 0px">
                    <div class="col-md-12">
                        <div class="top-section">
                            <div class="profile-image">
                                <img src="img/Dejan04crop.jpg" alt="Dejan">
                            </div>
                            <div class="profile-content">
                                <h3 class="profile-title">Dejan Milojevic</h3>
                                <p class="profile-description">Postdoctoral researcher and lecturer @ ETH Zürich</p>
                                <p class="profile-description"><br /></p>
                                <div class="social-icons">
                                    <ul>
                                        <li><a href="mailto:dejanmi@ethz.ch" target="_blank"><i class="fa fa-envelope"></i></a></li>
                                        <li><a href="https://www.linkedin.com/in/dejan-milojevic/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                                        <li><a href="https://scholar.google.com/citations?user=kEL5wk4AAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar"></i></a></li>
                                        <li><a href="https://github.com/dejanmi" target="_blank"><i class="fa fa-github"></i></a></li>
                                    </ul>
                                </div> <!-- .social-icons -->
                            </div>
                        </div>
                    </div>
                </div>
                <a href="#" class="toggle-menu"><i class="fa fa-bars"></i></a>
                <div class="main-navigation responsive-menu">
                    <ul class="navigation">
                        <li><a href="index.html"><i class="fa fa-home fa-fw"></i>Home</a></li>
                        <li><a href="research.html"><i class="fa fa-search fa-fw"></i>Research</a></li>
                        <li><a href="publications.html"><i class="fa fa-file fa-fw"></i>Publications</a></li>
                        <li><a href="education.html"><i class="fa fa-graduation-cap fa-fw"></i>Education</a></li>
                        <li><a href="experience.html"><i class="fa fa-suitcase fa-fw"></i>Experience</a></li>
                        <li><a href="advising.html"><i class="fa fa-users fa-fw"></i>Advising</a></li>
                        <li><a href="teaching.html"><i class="fa fa-book fa-fw"></i>Teaching</a></li>
                        <!-- <li><a href="awards.html"><i class="fa fa-star fa-fw"></i>Awards</a></li> -->
                    </ul>
                </div>
            </div>
        </div>

        <!-- SIDEBAR -->
        <div class="sidebar-menu hidden-xs hidden-sm">
            <div class="top-section">
                <div class="profile-image">
                    <img src="img/Dejan04crop.jpg" alt="Dejan">
                </div>
                <h3 class="profile-title">Dejan Milojevic</h3>
                <!-- <p class="profile-description">borisi (at) stanford (dot) edu</p> -->
                <p class="profile-description"><br style="color: #b09f95;"/></p>
                <!-- <p class="profile-description">Postdoctoral researcher and lecturer @ ETH Zürich</p> -->
                <p class="profile-description"><a href="https://idsc.ethz.ch/research-frazzoli/people/person-detail.MTg2NzM0.TGlzdC8yNjg5LDQ4ODg4MTE2Mw==.html" target="_blank">Postdoctoral researcher and lecturer</a></p>
                <p class="profile-description" style="color: #b09f95;"><a href="https://idsc.ethz.ch/research-frazzoli/research-projects.html" target="_blank">Research Frazzoli</a></p>
                <p class="profile-description" style="color: #b09f95;"><a href="https://idsc.ethz.ch/" target="_blank">Institute for Dynamic Systems and Control</a></p>
                <p class="profile-description" style="color: #b09f95;"><a href="https://ethz.ch/en.html" target="_blank">ETH Zürich</a></p>
            </div> <!-- top-section -->
            <div class="social-icons">
                <ul>
                    <li><a href="mailto:dejanmi@ethz.ch" target="_blank"><i class="fa fa-envelope"></i></a></li>
                    <li><a href="https://www.linkedin.com/in/dejan-milojevic/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                    <li><a href="https://scholar.google.com/citations?user=kEL5wk4AAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar"></i></a></li>
                    <li><a href="https://github.com/dejanmi" target="_blank"><i class="fa fa-github"></i></a></li>
                </ul>
            </div> <!-- .social-icons -->
            <div class="main-navigation">
                <ul class="navigation">
                    <li><a href="index.html"><i class="fa fa-home fa-fw"></i>Home</a></li>
                    <li><a href="research.html"><i class="fa fa-search fa-fw"></i>Research</a></li>
                    <li><a href="publications.html"><i class="fa fa-file fa-fw"></i>Publications</a></li>
                    <li><a href="education.html"><i class="fa fa-graduation-cap fa-fw"></i>Education</a></li>
                    <li><a href="experience.html"><i class="fa fa-suitcase fa-fw"></i>Experience</a></li>
                    <li><a href="advising.html"><i class="fa fa-users fa-fw"></i>Advising</a></li>
                    <li><a href="teaching.html"><i class="fa fa-book fa-fw"></i>Teaching</a></li>
                    <!-- <li><a href="awards.html"><i class="fa fa-star fa-fw"></i>Awards</a></li> -->
                </ul>
            </div> <!-- .main-navigation -->
        </div> <!-- .sidebar-menu -->

        <div class="banner-bg" id="research-banner" style="padding: 250px 300px;">
            <div class="banner-overlay">
              <img src="img/codeiwider.png" alt="research" style="object-position: center 50%; opacity: 1.0;">
            </div>
        </div>

        <!-- MAIN CONTENT -->
        <div class="main-content">
            <div class="fluid-container">
                <div class="content-wrapper">
                    <!-- UPDATES -->
                    <div class="page-section" id="about">
                    <div class="row">
                    <div class="row">
                        <div class="col-md-12">
                            <div class="quick-access">
                                <h4 class="widget-title">Research</h4>
                                <ul>
                                    <li><a href="#motivation-section">Motivation</a></li>
                                    <li><a href="#problem-formulation-section">Problem Formulation</a></li>
                                    <li><a href="#perception-requirement-section">Perception Requirements for Agents</a></li>
                                    <li><a href="#benchmarking-perception-section">Benchmarking Perception Pipelines</a></li>
                                    <li><a href="#sensor-selection-section">Sensor Selection and Placement Problem</a></li>
                                    <li><a href="#co-design-section">Co-design of Mobile Robots</a></li>
                                    <li><a href="#case-studies-section">Case Study</a></li>
                                </ul>
                            </div>
                            <!-- VIDEO EMBEDDED -->
                            <div class="row" style="margin-top: 20px;"> <!-- No extra top margin -->
                                <div class="col-md-12">
                                    <h4 class="widget-title"></h4>
                                    <div class="video-container" style="max-width: 800px; height: auto; margin: 0 auto; text-align: center;">
                                        <iframe style="width: 100%; height: 450px; max-width: 800px;" 
                                        src="https://www.youtube.com/embed/1PiJZz70oh4?si=koN56KJAZtm0pbTj" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                </div>
                            </div>       
                            <div class="header-quote-container" id="motivation-section">
                                <h5 class="widget-title">Motivation</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="The discovery of an important need is almost as important as the invention which satisfies this need.">
                                    <p id="animated-citation-pupin"></p>
                                    <p class="citation-author">- Mihajlo Idvorski Pupin</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text">
                                    Robots are revolutionizing industries by performing critical tasks in manufacturing, transportation, and inspection, significantly increasing production efficiency in sectors such as logistics and agriculture.                                </p>
                                <p class="content-text">
                                    However, to truly unlock their potential, we face <strong>the dual challenge of ensuring the robots can efficiently perform their designated tasks while also minimizing the resources to design these robots</strong>, such as costs or power consumption. This balance is crucial for maintaining profitability and competitiveness.
                                </p>
                                <p class="content-text">
                                    This brings us to a pivotal question: <strong>how can we automate the design of these robots to ensure they are task-efficient and resource-efficient?</strong>
                                </p>
                                <p class="content-text">
                                    My research aims to address this challenge, paving the way for the next generation of <strong>smart, efficient, and economically viable mobile robots</strong>.
                                </p>
                                <p class="content-text">
                                    Designing mobile robots presents a variety of complex challenges. <strong>One major challenge is the integration of numerous interconnected components</strong>. Consider a Mars rover: it has six wheels, an arm, and multiple sensors for navigation and scientific measurements. Each component requires hardware and software that must work together. For instance, sensors and actuators need a power supply from the battery, and the software requires computational resources from a computing unit.
                                </p>
                                <figure class="content-image">
                                    <a href="https://commons.wikimedia.org/wiki/File:NASA_Mars_Rover.jpg" title="NASA/JPL/Cornell University, Maas Digital LLC, Public domain, via Wikimedia Commons">
                                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/NASA_Mars_Rover.jpg/512px-NASA_Mars_Rover.jpg?20090103005528" alt="Artist's portrayal of Opportunity">
                                    </a>
                                    <figcaption>
                                        Image by <a href="https://commons.wikimedia.org/wiki/User:NASA/JPL/Cornell_University" target="_blank">NASA/JPL/Cornell University</a>, Maas Digital LLC, Public Domain, via <a href="https://commons.wikimedia.org/wiki/File:NASA_Mars_Rover.jpg" target="_blank">Wikimedia Commons</a>.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    <strong>Another challenge is the overwhelming number of design options available</strong>. Most components, such as the choice of a lidar sensor for your robot, are typically off-the-shelf items from catalogs. Beyond selecting the right lidar, designers must also decide on sensor placement and the integration of other sensor types, such as cameras and radars. Additionally, decisions must be made regarding software and actuators.
                                </p>
                                <figure class="content-image">
                                    <img src="img/autonomousstuff.png" alt="Screenshot of AutonomouStuff's product catalog, September 16, 2024.">
                                    <figcaption>
                                        Screenshot taken from <a href="https://autonomoustuff.com/products?para1=LiDAR" target="_blank">AutonomouStuff</a>. All images © AutonomouStuff.                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    <strong>Designing a robot also involves numerous trade-offs</strong>. For example, consider the various types of vacuum cleaning robots available for home use. These robots differ in functionalities, such as vacuum cleaning, wet cleaning, and battery life. The more functionalities a robot has, the higher the cost. My goal is to solve the design problem for a robot by ensuring it provides specific functionalities, such as cleaning a room with a certain area in square meters.
                                </p>
                                <figure class="content-image">
                                    <img src="https://images.squarespace-cdn.com/content/v1/574b35068a65e20cc74c4def/1520006085711-8PSZBSN6E2PGIOH8QIIK/RoombaComparison.jpg" alt="Roomba comparison chart">
                                    <figcaption>
                                        Image originally by iRobot (Roomba). Found on <a href="https://www.squarespace.com" target="_blank">Source Website</a>.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Achieving these functionalities requires resources such as cost and mass. The higher the performance required, the more resources are needed. For instance, cleaning a larger room requires a bigger, more expensive, and heavier battery. 
                                    Therefore, <strong>my aim is to identify design choices, known as implementations, that provide the necessary functionalities while minimizing resource use</strong>, or that offer the best possible functionality within given resource constraints.
                                </p>
                            </div>
                            <div class="header-quote-container" id="problem-formulation-section">
                                <h5 class="widget-title">Problem Formulation</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="The formulation of the problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill.">
                                    <p id="animated-citation-einstein"></p>
                                    <p class="citation-author">- Albert Einstein</p>
                                </div>
                            </div>                
                            <div class="content-wrapper research-page">
                                <p class="content-text">
                                    The following problem is addressed in the context of designing an autonomous vehicle. We begin by defining the task the robot must perform. Understanding the specific task is crucial for identifying a resource-efficient design.
                                </p>
                                <figure class="content-image">
                                    <img src="img/milojevic-tro.jpg" alt="Graphical illustration of the informal problem definition for design-
                                    ing an AV for urban driving tasks, based on a catalog of hardware
                                    and software components with an emphasis on minimizing resources.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, published in <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank">IEEE Transactions on Robotics</a>, 2025
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Next, we turn to a catalog of software options. On one hand, there are object detection algorithms that analyze sensor data, such as images or lidar point clouds, to detect objects such as cars and pedestrians. These perception algorithms, often based on deep neural networks, are crucial for enabling the vehicle to understand its environment. On the other hand, we have motion planners, which are responsible for calculating the robot's trajectories or actions to achieve its goals.
                                </p>
                                <p class="content-text">
                                    We also examine catalogs of hardware components, including vehicle bodies with predefined sensor mounting positions, a variety of sensors such as cameras and lidars, and computing units.
                                </p>
                                <p class="content-text">
                                    Our goal in using these catalogs is to identify a robot design that meets the task requirements while minimizing resource use. The final output, for example, could be a Pareto front of resources and their corresponding implementations. This enables us to visualize trade-offs and select the most efficient design.                                </p>
                            </div>
                            <div class="publication-item">
                                <b class="paper-title">CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, G. Zardini, M. Elser, A. Censi, E. Frazzoli, IEEE Transactions on Robotics, 2025.
                                    </span>
                                    <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank" class="publication-link">[PDF]</a>
                                        <a href="https://arxiv.org/abs/2503.10296" target="_blank" class="publication-link">[arXiv]</a>
                                    </div>
                                </div>
                                <b class="paper-title">Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, Doctoral Thesis. Supervised by Prof. Emilio Frazzoli, <i>Institute for Dynamic Systems and Control, ETH Zürich, 2024</i>.
                                    </span>
                                    <!-- <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/672201/task-driven-codesign-robots-20240509.pdf?sequence=1&isAllowed=y" target="_blank" class="publication-link">[PDF]</a>
                                    </div> -->
                                </div>
                            </div>
                            <div class="header-quote-container" id="perception-requirement-section">
                                <h5 class="widget-title">Perception Requirements for Agents</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="Planning is simply the result of experience read backward and projected into the future.">
                                    <p id="animated-citation-gregory"></p>
                                    <p class="citation-author">- William King Gregory</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text"> 
                                    Now, we embark on a critical aspect of our journey: understanding the perception requirements for autonomous agents. 
                                    Perception is the cornerstone of a mobile robot's ability to navigate and interact with its environment. 
                                    By determining the information needed by the motion planner, we can define the perception requirements for our system. 
                                </p>
                                <figure class="content-image">
                                    <img src="img/ZAM_Urban-3_1_uniform_cost.gif" alt="Graphical illustration of the prior.">
                                    <figcaption>
                                        Animation created with <a href="https://commonroad.in.tum.de/" target="_blank">CommonRoad</a>, 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text"> 
                                    A specific type of motion planner is the <strong>sampling-based motion planner</strong>. These planners bypass the need for a mathematical representation of the obstacle-free configuration space. Instead, they sample random configurations and incrementally build a representation of this space. There are various sampling strategies, each with its unique approach. For example, consider a <strong>lattice planner</strong> that uses motion primitives. Motion primitives are predefined trajectories projected onto the workspace to check for collisions. These collision checks, known as <strong>occupancy queries</strong>, are vital for determining perception requirements. Essentially, for each sampled configuration, the agent asks whether the configuration is in collision. If it is not, the configuration is feasible and can be included in the free configuration space. This sampling continues until the goal is reached or the solution is deemed satisfactory.
                                    An occupancy query is essentially a tuple that depends on the agent's configuration and the future time at which it is sampled. In the left image below, we see a lattice planner using motion primitives. While this motion planner is not optimal, it effectively illustrates the concept. 
                                    On the other hand, the <strong>RRT*</strong> planner, shown in the right image, is an asymptotically optimal sampling-based motion planner. From this comparison, we can see that different sampling or planning approaches require different information from the 
                                    perception system. For instance, the RRT* planner needs to check occupancy queries behind the vehicle, whereas the lattice planner does not. This highlights a <strong>trade-off between information requirements and optimality</strong>. 
                                </p>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/smart_plan_astar.png" alt="Example of a lattice planner, paired with motion primitives and A* search.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/smart_plan_rrt.png" alt="Example of an RRT ∗ planner.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <p class="content-text">
                                    Our goal is to identify the statistical characteristics of the possible occupancy queries generated by a motion planner for a specific task. To achieve this, we define the <strong>robot’s task as a set of scenarios</strong>, 
                                    which can also be represented as a distribution of scenarios. Through simulating these scenarios, we collect the generated occupancy queries and store them in a comprehensive database. 
                                    Systematically gathering these queries allows us to gain a deeper understanding of the perception requirements necessary for the robot to navigate safely and efficiently across all potential situations it may encounter.
                                </p>
                                <figure class="content-image">
                                    <video controls width="100%">
                                        <source src="video/sim.mp4" type="video/mp4">
                                        <source src="video/sim.webm" type="video/webm"> 
                                        <source src="video/sim.ogv" type="video/ogg"> 
                                        Your browser does not support the video tag.
                                    </video>
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Video of a simulation. The simulations are animated with <a href="https://www.blender.org/" target="_blank">Blender</a> by using the logs (vehicle configurations at 
                                        each simulation step) and the map from <a href="https://commonroad.in.tum.de/" target="_blank">CommonRoad</a>.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    For each scenario, we leverage <strong>prior knowledge about where obstacles are likely to be</strong>. This concept is essential for determining the perception requirements of our autonomous agent. Imagine if vehicles could appear from anywhere—sidewalks, rooftops, or even falling from the sky. 
                                Such a scenario would require a comprehensive sensor system capable of monitoring everything. However, by understanding where objects are likely to be, we can strategically position sensors to optimize performance and efficiency.
                                For example, on a highway, we can predict that vehicles will not approach us head-on in our lane. Similarly, for a warehouse robot, we might know that one area is frequented by humans while another is used exclusively by robots. 
                                This prior knowledge allows us to refine our perception system, focusing sensor coverage where it is most needed and reducing unnecessary redundancy.
                                By incorporating this knowledge into our design, we can tailor our sensor selection and placement more effectively, ensuring that our autonomous agents are both efficient and well-suited to their specific environments.                            
                                </p>
                                <figure class="content-image">
                                    <img src="img/prior.png" alt="Graphical illustration of the prior.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Given our understanding of occupancy queries and our prior knowledge of where objects are likely to be, we can now define the perception requirements for our autonomous vehicle.
                            Consider a small red autonomous vehicle starting from configuration \(q_0\) at time \(t_0\), with the goal of reaching the yellow area.                            
                                </p>
                                <figure class="content-image">
                                    <img src="img/perc-req-agent-goal.png" alt="Graphical illustration of an agent's task.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    The motion planner generates the following yellow trajectory with an occupancy query at configuration \(q_i\) at a future time step \(t_i\), indicated with the transparent red car.
                            The key question is: <strong>where should the sensors focus to accurately respond to this occupancy query?</strong>                         
                                </p>
                                <figure class="content-image">
                                    <img src="img/perc-req-agent-query-traj.png" alt="Graphical illustration of an agent's query and trajectory.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    It is similar to how we humans drive. The sensors need to detect all obstacles that could potentially collide with the vehicle at configuration \(q_i\) 
                                    in the future at time \(t_i\). This involves identifying all possible obstacle trajectories that could lead to a collision at \(t_i\).                         
                                </p>
                                <figure class="content-image">
                                    <img src="img/perc-req-agent-objj-traj.png" alt="Graphical illustration of potential object trajectories.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Now, our prior knowledge comes into play. Among the potential trajectories, some may be deemed infeasible based on the prior knowledge. 
                                    For example, the red car following the red trajectory is known to be infeasible because it is not permitted to turn left. 
                                    In contrast, the green trajectories are feasible according to our prior knowledge.                         
                                </p>
                                <figure class="content-image">
                                    <img src="img/perc-req-agent-objj-prior.png" alt="Graphical illustration of potential object trajectories.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Therefore, to answer the motion planner's occupancy query at time \(t_i\), the perception system needs to detect the bus and the black car. 
                                    By conducting this analysis for all generated occupancy queries related to the robot’s task, we can identify the object configurations 
                                    that the perception system must be able to detect.                     
                                </p>
                                <figure class="content-image">
                                    <img src="img/perc-req-agent-objj-prior-check.png" alt="Graphical illustration of potential object trajectories.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                            </div>
                            <div class="publication-item">
                                <b class="paper-title">CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, G. Zardini, M. Elser, A. Censi, E. Frazzoli, IEEE Transactions on Robotics, 2025.
                                    </span>
                                    <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank" class="publication-link">[PDF]</a>
                                        <a href="https://arxiv.org/abs/2503.10296" target="_blank" class="publication-link">[arXiv]</a>
                                    </div>
                                </div>
                                <b class="paper-title">Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, Doctoral Thesis. Supervised by Prof. Emilio Frazzoli, <i>Institute for Dynamic Systems and Control, ETH Zürich, 2024</i>.
                                    </span>
                                    <!-- <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/672201/task-driven-codesign-robots-20240509.pdf?sequence=1&isAllowed=y" target="_blank" class="publication-link">[PDF]</a>
                                    </div> -->
                                </div>
                            </div>
                            <div class="header-quote-container" id="benchmarking-perception-section">
                                <h5 class="widget-title">Benchmarking Perception Pipelines</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="If it’s not tested, it’s broken.">
                                    <p id="animated-citation-eckel"></p>
                                    <p class="citation-author">- Bruce Eckel</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text"> 
                                    With a clear understanding of the perception requirements for our motion planners, the next crucial step is to benchmark our perception pipelines, which consist of combinations of sensors and their corresponding detection algorithms for processing sensor data. 
                                    Benchmarking enables us to evaluate how effectively the perception systems deliver the necessary information.
                                </p>
                                <p class="content-text"> 
                                    Our perception pipeline benchmarking process follows the following steps: we utilize <strong>real sensor data</strong> with annotations 
                                    from the open-source AV dataset, <a href="https://www.nuscenes.org/">Nuscenes</a>, along with pre-trained perception algorithms from <a href="https://github.com/open-mmlab/mmdetection3d">MMDetection3d</a>. 
                                    We run the inference process to obtain detections, which are categorized as <strong>False Negatives (FN)</strong>, <strong>False Positives (FP)</strong>, and <strong>True Positives (TP)</strong>. From these detections, we <strong>extract relevant features</strong>, such as the distance of the event relative to the sensor and environmental conditions, and store this data in a database.
                                </p>
                                <p class="content-text"> 
                                    Using this data, we perform a <strong>binary classification</strong> to estimate the False Negative Rate (FNR) and False Positive Rate (FPR):
                                </p>
                                <ul class="content-text">
                                    <li><strong>FNR</strong>: the probability that an object is present but not detected.</li>
                                    <li><strong>FPR</strong>: the probability that a detection is made when no object is present.</li>
                                </ul>
                                <p class="content-text"> 
                                    For our FNR and FPR models, we also estimate the uncertainty, providing a <strong>confidence interval</strong> for each estimate. This helps us gauge the model’s confidence in the FNR or FPR values. In our analysis, we adopt a worst-case approach, considering the maximum possible FNR or FPR to ensure that our perception system remains robust and reliable under the most challenging conditions.
                                </p>
                                <p class="content-text"> 
                                    Through this benchmarking process, we can assess the performance of our perception pipelines.
                                </p>
                                <figure class="content-image">
                                    <img src="img/perception-alg_page-0001.jpg" alt="Graphical illustration of the prior.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text"> 
                                    Let's examine some benchmarking results for lidar and camera sensors. On the left, we have a polar plot showing the FNR, 
                                    and on the right, a plot for the FPR for a perception pipeline using lidar sensors. The points represent <strong>object positions 
                                    relative to the sensor in polar coordinates</strong>, with color intensity indicating performance—darker colors represent 
                                    lower FNR or FPR. We observe that both <strong>FNR and FPR increase with distance</strong>. This is because fewer lidar points reach 
                                    distant objects, or the resolution decreases, making them harder to detect accurately.

                                    Similar plots are shown for the camera sensor. Unlike lidar, the camera has a limited field of view rather than 360 degrees. 
                                    As with lidar, performance decreases with distance. Additionally, at the edges of the camera’s field of view, 
                                    <strong>performance drops as objects may only be partially visible</strong>.

                                    These insights are crucial for understanding the strengths and limitations of each sensor type, allowing us to make informed decisions about sensor placement and usage in our perception system.
                                </p>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/gp_test_fnr_r_theta_upper_lidar (1).jpg" alt="Example of a lattice planner, paired with motion primitives and A* search.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/gp_test_fpr_r_theta_upper_lidar (1).jpg" alt="Example of an RRT ∗ planner.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/gp_test_fnr_r_theta_upper_cam.jpg" alt="Example of a lattice planner, paired with motion primitives and A* search.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/gp_test_fpr_r_theta_upper_cam (1).jpg" alt="Example of an RRT ∗ planner.">
                                        <figcaption>
                                            &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="publication-item">
                                    <b class="paper-title">CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, G. Zardini, M. Elser, A. Censi, E. Frazzoli, IEEE Transactions on Robotics, 2025.
                                    </span>
                                    <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank" class="publication-link">[PDF]</a>
                                        <a href="https://arxiv.org/abs/2503.10296" target="_blank" class="publication-link">[arXiv]</a>
                                    </div>
                                </div>
                                    <b class="paper-title">Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            <b>D. Milojevic</b>, Doctoral Thesis. Supervised by Prof. Emilio Frazzoli, <i>Institute for Dynamic Systems and Control, ETH Zürich, 2024</i>.
                                        </span>
                                        <!-- <div class="publication-links">
                                            <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/672201/task-driven-codesign-robots-20240509.pdf?sequence=1&isAllowed=y" target="_blank" class="publication-link">[PDF]</a>
                                        </div> -->
                                    </div>
                                    <b class="paper-title">Automated driving sensor testing vehicle</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            C. Hohl, <h5 class="life-item-title"><b>D. Milojevic</b></h5>, M. Elser, J. Zgraggen, N. Vulin, <i>Bundesamt für Strassen (<a href="https://www.astra.admin.ch/astra/de/home.html" target="_blank">ASTRA</a>) 2021</i>.
                                        </span>
                                        <div class="publication-links">
                                            <a href="files/HohlMilojevicTesting.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://plus.empa.ch/images/2021-04-26-autonomes-fahren-grosse-bilder/20210719_AutomatedDrivingSensorTestingVehicle.pdf" target="_blank" class="publication-link">[PDF]</a>
                                            <a href="https://www.youtube.com/watch?v=SHwQyKMxHpY" target="_blank" class="publication-link">[video]</a>
                                        </div>
                                    </div>
                                    <b class="paper-title">Co-design of embodied intelligence: A structured approach</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            G. Zardini, <h5 class="life-item-title"><b>D. Milojevic</b></h5>, A. Censi, E. Frazzoli, <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (<a href="https://www.ieee-ras.org/component/rseventspro/event/2021-iros-2021" target="_blank">IROS</a>) 2021</i>.
                                        </span>
                                        <div class="publication-links">
                                            <a href="files/ZardiniMilojevicIROS2021.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://arxiv.org/pdf/2011.10756" target="_blank" class="publication-link">[PDF]</a>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="header-quote-container" id="sensor-selection-section">
                                <h5 class="widget-title">Sensor Selection and Placement Problem</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="There are no solutions, only trade-offs.">
                                    <p id="animated-citation-eckel"></p>
                                    <p class="citation-author">- Thomas Sowell</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text"> 
                                    With a method for quantifying the perception requirements for a motion planner and evaluating the performance of various perception pipelines, we can now formulate an optimization problem. The goal of this problem is to identify the perception pipelines that meet the necessary requirements while minimizing resource usage.
                                </p>
                                <p class="content-text"> 
                                    We begin by defining the <strong>task, represented as a set of scenarios</strong>.

                                    Next, using predefined vehicle bodies and motion planners, we simulate these scenarios to gather the generated occupancy queries.

                                    By integrating our prior knowledge of object configurations with the occupancy queries, we establish the perception requirements.

                                    In parallel, we benchmark the perception pipelines, check for occlusions, and evaluate the performance of the mounted sensors.

                                    Using a given grid, we compress and discretize both the perception coverage and the perception requirements.

                                    Finally, we solve the sensor selection and placement problem as a <strong>set cover problem</strong>. The result is a set of selected sensors and perception pipelines, including their mounting positions and orientations, that minimize specific costs such as price, mass, power consumption, or computing resources.

                                    This approach ensures that our autonomous system is optimized for both performance and efficiency, meeting all necessary perception requirements while minimizing resource usage.
                                </p>
                                <figure class="content-image">
                                    <img src="img/sensorselectionalg-upd-v1-gs.jpg" alt="Graphical illustration of the prior.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich, 2024.
                                    </figcaption>
                                </figure>
                                <div class="publication-item">
                                    <b class="paper-title">CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, G. Zardini, M. Elser, A. Censi, E. Frazzoli, IEEE Transactions on Robotics, 2025.
                                    </span>
                                    <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank" class="publication-link">[PDF]</a>
                                        <a href="https://arxiv.org/abs/2503.10296" target="_blank" class="publication-link">[arXiv]</a>
                                    </div>
                                </div>
                                    <b class="paper-title">Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            <b>D. Milojevic</b>, Doctoral Thesis. Supervised by Prof. Emilio Frazzoli, <i>Institute for Dynamic Systems and Control, ETH Zürich, 2024</i>.
                                        </span>
                                        <!-- <div class="publication-links">
                                            <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/672201/task-driven-codesign-robots-20240509.pdf?sequence=1&isAllowed=y" target="_blank" class="publication-link">[PDF]</a>
                                        </div> -->
                                    </div>
                                </div>
                            </div>
                            <div class="header-quote-container" id="co-design-section">
                                <h5 class="widget-title">Co-design of Mobile Robots</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="Civilization advances by extending the number of operations we can perform without thinking about them.">
                                    <p id="animated-citation-eckel"></p>
                                    <p class="citation-author">- Alfred North Whitehead</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text"> 
                                    We have explored how to select the optimal perception pipelines for a given task, motion planner, and vehicle. But what if our goal is to find the overall optimal design by also optimizing the vehicle body and motion planner itself?
                                </p>
                                <p class="content-text">
                                    To achieve an overall optimal design, including vehicle body and motion planner optimization, we leverage the <strong>monotone co-design theory</strong> developed by <a href="https://censi.science/">Dr. Andrea Censi</a>.
                                    For deeper insights into the theory I recommend the reference "<a href="https://applied-compositional-thinking.engineering/">Applied Compositional Thinking for Engineers</a>" by <a href="https://censi.science/">Andrea Censi</a>, <a href="https://lorand.earth/math">Jonathan Lorand</a> and <a href="https://zardini.mit.edu/">Gioele Zardini</a>.
                                </p>
                                <p class="content-text">
                                    In this approach, the <strong>design problem</strong> is modeled as a feasibility relation, where <span style="color: green;">functionalities</span> and <span style="color: red;">resources</span> are represented as <strong>partially ordered sets</strong>. For a detailed reference on partial orders, see <a href="https://doi.org/10.1017/CBO9780511809088">Davey and Priestly</a>. 
                                    This structure allows us to effectively model costs. For example, as in the earlier cleaning robot scenario, we prefer lower prices over higher ones. However, some costs may be incomparable; for instance, one robot may have lower costs but higher mass. 
                                    Design choices, such as a particular robot design—referred to as <strong>implementations</strong>—can be mapped to specific <span style="color: green;">functionalities</span> and <span style="color: red;">resources</span>.
                                </p>

                                <figure class="content-image">
                                    <img src="img/robotdesign-1.png" alt="The co-design diagram for the design of a mobile robot tailored to accomplish a specific task.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, published in <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank">IEEE Transactions on Robotics</a>, 2025.
                                    </figcaption>
                                </figure>
                                <h6 class="widget-title"><strong>Feasibility Relation</strong></h6>
                                <p class="content-text"> 
                                    For each design problem, we define a map that identifies the set of implementations for which <span style="color: green;">functionalities  \(f\)</span> are feasible with <span style="color: red;">resources \(r\)</span>. This is a monotone map, ensuring that requiring fewer functionalities does not demand more resources, and using more resources does not yield fewer functionalities.
                                </p>
                                <p class="content-text"> 
                                    This abstract framework can be represented <strong>diagrammatically</strong>, providing a clear visualization of the design space.
                                    <strong>How can engineers use this framework?</strong> Feasibility relations can be populated in several ways:
                                </p>
                                <ul class="content-text">
                                    <li><em>Catalogs</em>: as seen with the cleaning robot example, we use catalogs to compare price and functionalities.</li>
                                    <li><em>First Principles</em>: for instance, the energy needed to clean a room is always greater than or equal to the duration of the cleaning process times the power consumption of the robot.</li>
                                    <li><em>Data Points</em>: feasibility relations can also be derived from simulations or experiments.</li>
                                </ul>
                                <h6 class="widget-title"><strong>Solving the Optimization Problem</strong></h6>
                                <p class="content-text">
                                    We can solve two types of queries:
                                </p>
                                <ul class="content-text">
                                    <li><em>Fixed Functionality Query</em>: we fix the desired functionality and find design solutions that minimize resource usage.</li>
                                    <li><em>Fixed Budget Query</em>: we fix a resource budget and find implementations that maximize functionalities.</li>
                                </ul>

                                <figure class="content-image">
                                    <img src="img/codesignconnected14.jpg" alt="The co-design diagram for the design of a mobile robot tailored to accomplish a specific task.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, published in <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank">IEEE Transactions on Robotics</a>, 2025
                                    </figcaption>
                                </figure>

                                <p class="content-text"> 
                                    A powerful feature of this tool is its ability to <strong>compose</strong> and <strong>interconnect</strong> different design problems. For example, we can arrange design problems in <strong>series</strong> , in <strong>parallel</strong> , or even in <strong>feedback loops</strong> .

                                    When two design problems are combined, they form a new, integrated design problem. This flexibility allows us to address complex design challenges by breaking them down into manageable sub-problems, each of which can be optimized individually or in conjunction with others.

                                    By leveraging this compositional approach, we can systematically explore the design space, ensuring that our solutions are not only optimal but also robust and adaptable to various scenarios and requirements.
                                </p>
                                <div class="publication-item">
                                    <b class="paper-title">CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</b>
                                <div class="authors-and-links">
                                    <span class="authors">
                                        <b>D. Milojevic</b>, G. Zardini, M. Elser, A. Censi, E. Frazzoli, IEEE Transactions on Robotics, 2025.
                                    </span>
                                    <div class="publication-links">
                                        <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                        <a href="https://ieeexplore.ieee.org/document/10930687" target="_blank" class="publication-link">[PDF]</a>
                                        <a href="https://arxiv.org/abs/2503.10296" target="_blank" class="publication-link">[arXiv]</a>
                                    </div>
                                </div>
                                    <b class="paper-title">Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            <b>D. Milojevic</b>, Doctoral Thesis. Supervised by Prof. Emilio Frazzoli, <i>Institute for Dynamic Systems and Control, ETH Zürich, 2024</i>.
                                        </span>
                                        <!-- <div class="publication-links">
                                            <a href="files/DejanMilojevic TRO.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/672201/task-driven-codesign-robots-20240509.pdf?sequence=1&isAllowed=y" target="_blank" class="publication-link">[PDF]</a>
                                        </div> -->
                                    </div>
                                    <b class="paper-title">Co-design of embodied intelligence: A structured approach</b>
                                    <div class="authors-and-links">
                                        <span class="authors">
                                            G. Zardini, <h5 class="life-item-title"><b>D. Milojevic</b></h5>, A. Censi, E. Frazzoli, <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (<a href="https://www.ieee-ras.org/component/rseventspro/event/2021-iros-2021" target="_blank">IROS</a>) 2021</i>.
                                        </span>
                                        <div class="publication-links">
                                            <a href="files/ZardiniMilojevicIROS2021.txt" target="_blank" class="publication-link">[BibTeX]</a>
                                            <a href="https://arxiv.org/pdf/2011.10756" target="_blank" class="publication-link">[PDF]</a>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="header-quote-container" id="case-studies-section">
                                <h5 class="widget-title">Case Study</h5>
                                <hr class="separator-line">
                                <div class="citation-container" data-citation="The true method of knowledge is experiment.">
                                    <p id="animated-citation-eckel"></p>
                                    <p class="citation-author">- William Blake</p>
                                </div>
                            </div>
                            <div class="content-wrapper research-page">
                                <p class="content-text"> 
                                    In this case study, we explore the design of an autonomous vehicle using urban driving scenarios from the <a href="https://commonroad.in.tum.de/" target="_blank">CommonRoad</a> dataset. 
                                    These scenarios, derived from real-world maps of various countries, provide a diverse and challenging environment for our vehicle.

                                    For object detection, we employ the <a href="https://github.com/open-mmlab/mmdetection3d">MMDetection3d</a> library, utilizing <strong>PointPillars</strong> for lidar and <strong>FCOS3D</strong> for cameras. 

                                    For motion planning, we use the <a href="https://ompl.kavrakilab.org/">OMPL</a> library, a widely used open-source library for sampling-based motion planning algorithms. 
                                    Specifically, we consider three planners: a <strong>lattice planner with A* search</strong>, the <strong>RRT</strong> planner, and the <strong>RRT*</strong> planner.

                                    Our vehicle body options include three types: a small <strong>hatchback, a van, and a sedan</strong>. To enhance sensing capabilities, we have around ten different sensors, including lidars and cameras with various resolutions and specifications. 
                                    Additionally, we have a selection of ten computers with varying levels of computing power.

                                    It is important to note that designers are free to generate their own catalogs of software and hardware components.
                                </p>
                                <figure class="content-image">
                                    <img src="img/catalog.png" alt="Graphical illustration of the AV co-design catalog.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from Dejan Milojevic's doctoral thesis: <em>Co-design of Mobile Robots - Integrating Perception Systems and Motion Planning for Task Specific Optimization</em>, ETH Zürich , 2024.
                                    </figcaption>
                                </figure>
                                <p class="content-text">
                                    Let's explore how the number of scenarios in a task impacts resource usage in the design of our autonomous vehicle. <strong>More scenarios generally require more resources</strong>, as demonstrated in the following series of plots.
                                    The first plot shows Pareto fronts comparing the total mass (in kilograms) of selected sensors and computers against the total cost (in Swiss francs). Different shades of red represent varying numbers of scenarios:
                                </p>
                                <ul class="content-text">
                                    <li>Dark red: 205 scenarios.</li>
                                    <li>Brightest red: 9 scenarios.</li>
                                </ul>
                                <p class="content-text"></p>
                                Each point represents an optimal solution, and the red area highlights the feasible resource range, indicating that similar solutions may demand more resources as the task complexity increases. The design choices with the lowest mass for the maximum and minimum tasks are also shown. Triangles represent the orientation of cameras, while squares represent lidars, with the same perception pipeline color-coded.
                                Cameras are prioritized due to their lighter weight compared to lidars. The most powerful computing unit is required because camera-based perception algorithms, which typically involve large neural networks with more parameters, demand higher computational power.
                                </p>
                                <figure class="content-image">
                                    <img src="img/scenarios_fixed_costs_mass-1.png" alt="Pareto front for different tasks comparing mass and price.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                    </figcaption>
                                </figure>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_50.0_dry_day_mass_1.7_109404.0-1.png" alt="Implementation of the lightest design for the most difficult task.">
                                        <figcaption>
                                            (A) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_AA_50.0_dry_day_mass_0.91_55774.0-1.png" alt="Implementation of the lightest design for the simplest task.">
                                        <figcaption>
                                            (B) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <p class="content-text">
                                    Nominal speed in scenarios has a significant impact on resource usage. <strong>Higher nominal speeds require more resources</strong> for several reasons:
                                </p>
                                <ul class="content-text">
                                    <li>Motion planners generate longer-range trajectories.</li>
                                    <li>Sensors need to cover a greater range to detect objects at higher speeds.</li>
                                </ul>
                                <p class="content-text">
                                    The following plot shows the Pareto fronts for total power consumption of the selected sensors and computing units (in watts) and total cost (in Swiss francs) for different nominal speeds.
                                    The plot demonstrates how increasing nominal speed demands additional resources.
                                </p>

                                <figure class="content-image">
                                    <img src="img/velocity_fixed_costs_power_dry_day-1.png" alt="Pareto front for different nominal speeds comparing power consumption and price.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                    </figcaption>
                                </figure>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_50.0_dry_day_power_40.1_65355.0_289-1.png" alt="Implementation of the lowest power consumption design for the highest nominal speed.">
                                        <figcaption>
                                            (A) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_30.0_dry_day_power_23.0_114125.0-1.png" alt="Implementation of the lowest power consumption design for the lowest nominal speed.">
                                        <figcaption>
                                            (B) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <p class="content-text">
                                    Prior knowledge of where objects can be located in scenarios also influences resource consumption. <strong>More potential object positions increase perception requirements, demanding additional or higher-quality sensors and perception algorithms</strong>.

                                    In this study, we restricted the prior knowledge, preventing vehicles from approaching from the left or behind. 
                                    <strong>This illustrates how limiting possible object positions can reduce resource needs</strong>.
                                </p>
                                <figure class="content-image">
                                    <img src="img/velocity_prior_fixed_costs_mass_dry_day-1.png" alt="Pareto front for different priors comparing mass and price.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                    </figcaption>
                                </figure>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_50.0_dry_day_mass_1.7_109404.0-1.png" alt="Implementation of the lightest design for the full prior.">
                                        <figcaption>
                                            (A) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_no_left_behind_frontal_50.0_dry_day_mass_0.78_50694.0_2745-1.png" alt="Implementation of the lightest design for the restricted prior.">
                                        <figcaption>
                                            (B) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <p class="content-text">
                                    When solving the co-design problem, we can fix certain functionalities and identify implementations that minimize resource usage. 
                                    In this case, we explore the impact of <strong>requiring higher task performance</strong>, specifically a higher average speed.
                                    For higher average speeds, the results show that the only asymptotically stable motion planner, RRT*, is necessary, along with the vehicle body offering the highest acceleration—the sedan.
                                    In the image below we show the Pareto fronts for total computation needed by the perception and planning algorithms (in GFLOPS) and total cost (in Swiss francs) for different average speeds.
                                    From the implementation plots we can see that cameras are generally cheaper than lidars, making them the preferred choice in the least expensive designs.
                                </p>
                                <figure class="content-image">
                                    <img src="img/avg_speed_fixed_costs_computation-1.png" alt="Pareto front for different average speeds comparing computation need and price.">
                                    <figcaption>
                                        &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                    </figcaption>
                                </figure>
                                <div class="image-row">
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_6.8_dry_day_computation_1746.9_105734.0-1.png" alt="Implementation of the cheapest design for the highest average speed.">
                                        <figcaption>
                                            (A) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                            
                                    <figure class="image-column">
                                        <img src="img/implementation_task_urban_car_cr_6.8_dry_day_computation_136.8_121725.0-1.png" alt="Implementation of the least computation need design for the highest average speed.">
                                        <figcaption>
                                            (B) &copy; 2025 Dejan Milojevic. Image from our publication: <em>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</em>, 2024.
                                        </figcaption>
                                    </figure>
                                </div>
                                <h6 class="widget-title"><strong>Conclusion</strong></h6>
                                <ul class="content-text">
                                    <li>Higher average speeds require more sophisticated motion planners and vehicle bodies with better dynamics.</li>
                                    <li>Cameras are favored in designs prioritizing lower mass and cost.</li>
                                    <li>Lidars are preferred when minimizing computational resources.</li>
                                    <li>Overall, lidars are more powerful and are always selected for the most complex tasks.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                <hr>
                    </div>

                    <div class="row" id="footer">
                        <div class="col-md-12 text-center">
                            <p class="copyright-text">&copy; 2025 Dejan Milojevic</p>
                        </div>
                    </div>

                </div>

            </div>
        </div>

        <script src="js/vendor/jquery-1.10.2.min.js"></script>
        <script src="js/min/plugins.min.js"></script>
        <script src="js/min/main.min.js"></script>
        <script src="js/animated-citation.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    </body>
</html>
